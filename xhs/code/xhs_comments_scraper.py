import re
import os
import json
import time
import asyncio
from datetime import datetime
from playwright.async_api import async_playwright

class XHSCommentScraper:
    """å°çº¢ä¹¦è¯„è®ºçˆ¬å–å™¨"""
    
    def __init__(self, headless=True, timeout=30, debug=True):
        self.headless = headless
        self.timeout = timeout
        self.debug = debug
        self.user_agent = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
    
    def log(self, message, level="INFO"):
        """è°ƒè¯•æ—¥å¿—è¾“å‡º"""
        if self.debug:
            timestamp = datetime.now().strftime("%H:%M:%S")
            print(f"[{timestamp}] {level}: {message}")
    
    def extract_note_id(self, url: str) -> str | None:
        """ä»URLä¸­æå–ç¬”è®°ID"""
        match = re.search(r"/item/([a-z0-9]+)", url)
        note_id = match.group(1) if match else None
        self.log(f"æå–ç¬”è®°ID: {note_id}")
        return note_id
    
    def load_cookies(self, cookie_path: str | None):
        """åŠ è½½cookiesæ–‡ä»¶ - æ”¯æŒå¤šç§æ ¼å¼"""
        if not cookie_path:
            self.log("æœªæä¾›cookiesæ–‡ä»¶è·¯å¾„")
            return None
            
        if not os.path.exists(cookie_path):
            self.log(f"Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {cookie_path}")
            return None
            
        try:
            with open(cookie_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # æ”¯æŒå¤šç§cookiesæ ¼å¼
            cookies = None
            if isinstance(data, list):
                # ç›´æ¥æ˜¯cookieæ•°ç»„
                cookies = data
            elif isinstance(data, dict):
                if "cookies" in data:
                    # {cookies: [...]} æ ¼å¼
                    cookies = data["cookies"]
                elif "value" in data or "name" in data:
                    # å•ä¸ªcookieå¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„
                    cookies = [data]
                else:
                    # å¯èƒ½æ˜¯å…¶ä»–æ ¼å¼ï¼Œå°è¯•æ‰¾åˆ°cookieså­—æ®µ
                    for key, value in data.items():
                        if isinstance(value, list) and len(value) > 0 and "name" in value[0]:
                            cookies = value
                            break
            
            if not cookies:
                self.log("cookiesæ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º", "ERROR")
                return None
            
            # éªŒè¯cookiesæ ¼å¼
            valid_cookies = []
            for cookie in cookies:
                if isinstance(cookie, dict) and "name" in cookie and "value" in cookie:
                    # ç¡®ä¿å¿…è¦çš„å­—æ®µå­˜åœ¨
                    if "domain" not in cookie:
                        cookie["domain"] = ".xiaohongshu.com"
                    if "path" not in cookie:
                        cookie["path"] = "/"
                    valid_cookies.append(cookie)
            
            self.log(f"æˆåŠŸåŠ è½½ {len(valid_cookies)} ä¸ªæœ‰æ•ˆcookies")
            
            # æ˜¾ç¤ºé‡è¦cookiesä¿¡æ¯
            important_names = ['web_session', 'a1', 'webId', 'xsecappid']
            for cookie in valid_cookies:
                if cookie['name'] in important_names:
                    value_preview = cookie['value'][:20] + "..." if len(cookie['value']) > 20 else cookie['value']
                    self.log(f"  - {cookie['name']}: {value_preview}")
            
            return valid_cookies
            
        except Exception as e:
            self.log(f"åŠ è½½cookieså¤±è´¥: {e}", "ERROR")
            return None
    
    async def check_login_status(self, page):
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        self.log("æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        
        try:
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_timeout(2000)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•ç›¸å…³çš„å…ƒç´ 
            login_indicators = [
                "text=ç™»å½•", "text=æ³¨å†Œ", "text=ç«‹å³ç™»å½•",
                "[data-testid*='login']", "[class*='login']"
            ]
            
            has_login_button = False
            for indicator in login_indicators:
                try:
                    element = await page.query_selector(indicator)
                    if element and await element.is_visible():
                        has_login_button = True
                        break
                except:
                    continue
            
            # æ£€æŸ¥å½“å‰URL
            current_url = page.url
            is_login_page = any(keyword in current_url.lower() for keyword in ['login', 'signin', 'register'])
            
            # æ£€æŸ¥é¡µé¢å†…å®¹
            page_text = await page.evaluate("document.body.textContent")
            has_login_text = any(keyword in page_text for keyword in ['è¯·ç™»å½•', 'ç™»å½•', 'æ³¨å†Œ'])
            
            if has_login_button or is_login_page or has_login_text:
                self.log("âŒ æ£€æµ‹åˆ°æœªç™»å½•çŠ¶æ€", "WARNING")
                self.log(f"   å½“å‰URL: {current_url}")
                self.log(f"   ç™»å½•æŒ‰é’®: {has_login_button}")
                self.log(f"   ç™»å½•é¡µé¢: {is_login_page}")
                return False
            else:
                self.log("âœ… å·²ç™»å½•çŠ¶æ€")
                return True
                
        except Exception as e:
            self.log(f"æ£€æŸ¥ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}", "ERROR")
            return False
    
    async def analyze_page_structure(self, page):
        """åˆ†æé¡µé¢ç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•"""
        self.log("å¼€å§‹åˆ†æé¡µé¢ç»“æ„...")
        
        # è·å–é¡µé¢åŸºæœ¬ä¿¡æ¯
        title = await page.title()
        url = page.url
        self.log(f"é¡µé¢æ ‡é¢˜: {title}")
        self.log(f"å½“å‰URL: {url}")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦å®Œå…¨åŠ è½½
        ready_state = await page.evaluate("document.readyState")
        self.log(f"é¡µé¢åŠ è½½çŠ¶æ€: {ready_state}")
        
        # åˆ†æé¡µé¢ä¸­åŒ…å«"è¯„è®º"ç›¸å…³çš„å…ƒç´ 
        analysis_js = """
        (() => {
            const info = {
                totalElements: document.querySelectorAll('*').length,
                commentKeywords: [],
                possibleCommentContainers: [],
                textContent: document.body.textContent.length,
                hasLoginButton: false,
                hasCommentSection: false,
                pageContent: document.body.textContent.substring(0, 500)
            };
            
            // æŸ¥æ‰¾åŒ…å«è¯„è®ºå…³é”®è¯çš„å…ƒç´ 
            const keywords = ['è¯„è®º', 'comment', 'å›å¤', 'reply', 'ç‚¹èµ', 'like'];
            keywords.forEach(keyword => {
                const elements = Array.from(document.querySelectorAll('*')).filter(el => {
                    return el.textContent && el.textContent.toLowerCase().includes(keyword.toLowerCase());
                });
                if (elements.length > 0) {
                    info.commentKeywords.push({
                        keyword: keyword,
                        count: elements.length,
                        samples: elements.slice(0, 3).map(el => ({
                            tagName: el.tagName,
                            className: el.className,
                            textContent: el.textContent.substring(0, 100)
                        }))
                    });
                }
            });
            
            // æŸ¥æ‰¾å¯èƒ½çš„è¯„è®ºå®¹å™¨
            const containerSelectors = [
                '[class*="comment"]', '[id*="comment"]',
                '[class*="Comment"]', '[id*="Comment"]',
                'section', 'div[class*="list"]',
                '[data-testid*="comment"]',
                '[class*="interaction"]', '[class*="note-detail"]'
            ];
            
            containerSelectors.forEach(selector => {
                try {
                    const elements = document.querySelectorAll(selector);
                    if (elements.length > 0) {
                        info.possibleCommentContainers.push({
                            selector: selector,
                            count: elements.length,
                            samples: Array.from(elements).slice(0, 2).map(el => ({
                                tagName: el.tagName,
                                className: el.className,
                                id: el.id,
                                textLength: el.textContent ? el.textContent.length : 0
                            }))
                        });
                    }
                } catch (e) {}
            });
            
            // æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•ç›¸å…³å…ƒç´ 
            const loginKeywords = ['ç™»å½•', 'login', 'ç™»é™†', 'sign in', 'è¯·ç™»å½•'];
            loginKeywords.forEach(keyword => {
                const loginElements = Array.from(document.querySelectorAll('*')).filter(el => {
                    return el.textContent && el.textContent.toLowerCase().includes(keyword.toLowerCase());
                });
                if (loginElements.length > 0) {
                    info.hasLoginButton = true;
                }
            });
            
            // æ£€æŸ¥æ˜¯å¦æœ‰è¯„è®ºåŒºåŸŸ
            const commentSections = document.querySelectorAll('[class*="comment"], [id*="comment"]');
            info.hasCommentSection = commentSections.length > 0;
            
            return info;
        })();
        """
        
        try:
            analysis = await page.evaluate(analysis_js)
            
            self.log(f"é¡µé¢æ€»å…ƒç´ æ•°: {analysis['totalElements']}")
            self.log(f"é¡µé¢æ–‡æœ¬é•¿åº¦: {analysis['textContent']}")
            self.log(f"æ˜¯å¦æ£€æµ‹åˆ°ç™»å½•æŒ‰é’®: {analysis['hasLoginButton']}")
            self.log(f"æ˜¯å¦æ£€æµ‹åˆ°è¯„è®ºåŒºåŸŸ: {analysis['hasCommentSection']}")
            
            # æ˜¾ç¤ºé¡µé¢å†…å®¹é¢„è§ˆ
            self.log(f"é¡µé¢å†…å®¹é¢„è§ˆ: {analysis.get('pageContent', '')[:200]}...")
            
            self.log("è¯„è®ºå…³é”®è¯åˆ†æ:")
            for item in analysis['commentKeywords']:
                self.log(f"  - '{item['keyword']}': {item['count']} ä¸ªå…ƒç´ ")
                for sample in item['samples']:
                    self.log(f"    * {sample['tagName']}.{sample['className']}: {sample['textContent'][:50]}...")
            
            self.log("å¯èƒ½çš„è¯„è®ºå®¹å™¨:")
            for container in analysis['possibleCommentContainers']:
                self.log(f"  - {container['selector']}: {container['count']} ä¸ªå…ƒç´ ")
                for sample in container['samples']:
                    self.log(f"    * {sample['tagName']}.{sample['className']} (æ–‡æœ¬é•¿åº¦: {sample['textLength']})")
            
            return analysis
            
        except Exception as e:
            self.log(f"é¡µé¢ç»“æ„åˆ†æå¤±è´¥: {e}", "ERROR")
            return None
    
    async def scroll_and_load_comments(self, page, max_rounds=30, sleep_sec=2.0):
        """æ»šåŠ¨é¡µé¢å¹¶åŠ è½½æ›´å¤šè¯„è®º"""
        self.log("å¼€å§‹æ»šåŠ¨åŠ è½½è¯„è®º...")
        last_height = 0
        stable_rounds = 0
        
        for round_num in range(max_rounds):
            self.log(f"ç¬¬ {round_num + 1}/{max_rounds} è½®æ»šåŠ¨")
            
            # è·å–å½“å‰é¡µé¢é«˜åº¦
            curr_height = await page.evaluate("document.body.scrollHeight")
            self.log(f"å½“å‰é¡µé¢é«˜åº¦: {curr_height}")
            
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(int(sleep_sec * 1000))
            
            # å°è¯•ç‚¹å‡»å„ç§"åŠ è½½æ›´å¤š"æŒ‰é’®
            load_more_texts = [
                "å±•å¼€æ›´å¤šè¯„è®º", "æŸ¥çœ‹å…¨éƒ¨è¯„è®º", "åŠ è½½æ›´å¤š", "æ›´å¤šè¯„è®º", 
                "å±•å¼€", "æ›´å¤š", "æŸ¥çœ‹æ›´å¤š", "ç‚¹å‡»æŸ¥çœ‹å…¨éƒ¨è¯„è®º", "æ˜¾ç¤ºæ›´å¤šè¯„è®º"
            ]
            
            clicked_button = False
            for text in load_more_texts:
                try:
                    # æŸ¥æ‰¾å¹¶ç‚¹å‡»æŒ‰é’®
                    buttons = await page.query_selector_all(f"text={text}")
                    for button in buttons:
                        if await button.is_visible():
                            await button.click(timeout=2000)
                            self.log(f"ç‚¹å‡»äº†'{text}'æŒ‰é’®")
                            await page.wait_for_timeout(1000)
                            clicked_button = True
                            break
                    if clicked_button:
                        break
                except Exception as e:
                    self.log(f"ç‚¹å‡»'{text}'æŒ‰é’®å¤±è´¥: {e}", "DEBUG")
                    continue
            
            # æ£€æŸ¥é¡µé¢é«˜åº¦å˜åŒ–
            new_height = await page.evaluate("document.body.scrollHeight")
            self.log(f"æ»šåŠ¨åé¡µé¢é«˜åº¦: {new_height}")
            
            if new_height == curr_height == last_height:
                stable_rounds += 1
                self.log(f"é¡µé¢é«˜åº¦æ— å˜åŒ– (è¿ç»­ {stable_rounds} è½®)")
                if stable_rounds >= 3:  # è¿ç»­3è½®æ— å˜åŒ–å°±åœæ­¢
                    self.log("é¡µé¢é«˜åº¦æ— å˜åŒ–ï¼Œåœæ­¢æ»šåŠ¨")
                    break
            else:
                stable_rounds = 0
                last_height = new_height
        
        self.log("æ»šåŠ¨åŠ è½½å®Œæˆ")
    
    async def extract_comments(self, page):
        """æå–è¯„è®ºæ•°æ®"""
        self.log("å¼€å§‹æå–è¯„è®ºæ•°æ®...")
        
        # å¢å¼ºçš„JavaScriptä»£ç ç”¨äºæå–è¯„è®º
        js_code = """
        (() => {
            const debugInfo = {
                searchResults: [],
                finalComments: [],
                errors: []
            };
            
            // å°è¯•å¤šç§è¯„è®ºé€‰æ‹©å™¨
            const selectors = [
                '[class*="comment"]',
                '[class*="Comment"]', 
                '[data-testid*="comment"]',
                '.comment-item',
                '.comment-list .comment',
                '[class*="note-comment"]',
                '[class*="NoteComment"]',
                '[class*="feed-comment"]',
                '[class*="user-comment"]',
                '[class*="interaction"]'
            ];
            
            let commentElements = [];
            
            // å°è¯•æ¯ä¸ªé€‰æ‹©å™¨å¹¶è®°å½•ç»“æœ
            for (const selector of selectors) {
                try {
                    const elements = document.querySelectorAll(selector);
                    debugInfo.searchResults.push({
                        selector: selector,
                        found: elements.length,
                        samples: Array.from(elements).slice(0, 2).map(el => ({
                            tagName: el.tagName,
                            className: el.className,
                            textLength: el.textContent ? el.textContent.length : 0,
                            textPreview: el.textContent ? el.textContent.substring(0, 50) : ''
                        }))
                    });
                    
                    if (elements.length > 0) {
                        commentElements = Array.from(elements);
                        break;
                    }
                } catch (e) {
                    debugInfo.errors.push(`é€‰æ‹©å™¨ ${selector} å‡ºé”™: ${e.message}`);
                }
            }
            
            // å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šé€‰æ‹©å™¨ï¼Œå°è¯•é€šè¿‡æ–‡æœ¬ç‰¹å¾æŸ¥æ‰¾
            if (commentElements.length === 0) {
                console.log('ä½¿ç”¨æ–‡æœ¬ç‰¹å¾æŸ¥æ‰¾è¯„è®º...');
                const allElements = document.querySelectorAll('div, section, article, span, p');
                const candidateElements = Array.from(allElements).filter(el => {
                    const text = el.textContent || '';
                    const className = el.className || '';
                    
                    // æ›´å®½æ³›çš„åŒ¹é…æ¡ä»¶
                    return (
                        text.length > 5 && text.length < 2000 && // åˆç†çš„è¯„è®ºé•¿åº¦èŒƒå›´
                        (className.toLowerCase().includes('comment') ||
                         text.includes('å›å¤') || text.includes('ç‚¹èµ') ||
                         text.includes('â¤ï¸') || text.includes('ğŸ‘') ||
                         text.includes('åˆ†é’Ÿå‰') || text.includes('å°æ—¶å‰') ||
                         text.includes('å¤©å‰') || text.includes('åˆšåˆš'))
                    );
                });
                
                commentElements = candidateElements;
                debugInfo.searchResults.push({
                    selector: 'text-feature-based',
                    found: candidateElements.length,
                    samples: candidateElements.slice(0, 5).map(el => ({
                        tagName: el.tagName,
                        className: el.className,
                        textLength: el.textContent ? el.textContent.length : 0,
                        textPreview: el.textContent ? el.textContent.substring(0, 50) : ''
                    }))
                });
            }
            
            // æå–è¯„è®ºä¿¡æ¯
            const comments = [];
            commentElements.forEach((element, index) => {
                try {
                    const textContent = element.textContent?.trim() || '';
                    
                    // è¿‡æ»¤æ¡ä»¶æ›´åŠ å®½æ¾
                    if (textContent.length < 2 || 
                        textContent.includes('å±•å¼€æ›´å¤š') ||
                        textContent.includes('æŸ¥çœ‹å…¨éƒ¨') ||
                        textContent === 'è¯„è®º' ||
                        textContent === 'ç‚¹èµ' ||
                        textContent === 'å›å¤' ||
                        textContent.includes('ç™»å½•') ||
                        textContent.includes('æ³¨å†Œ')) {
                        return;
                    }
                    
                    // å°è¯•æå–ç”¨æˆ·å
                    let username = '';
                    const userSelectors = '[class*="user"], [class*="name"], [class*="author"], [class*="nick"]';
                    const userElements = element.querySelectorAll(userSelectors);
                    if (userElements.length > 0) {
                        username = userElements[0].textContent?.trim() || '';
                    }
                    
                    // å°è¯•æå–æ—¶é—´
                    let timestamp = '';
                    const timeSelectors = '[class*="time"], time, [datetime], [class*="date"]';
                    const timeElements = element.querySelectorAll(timeSelectors);
                    if (timeElements.length > 0) {
                        timestamp = timeElements[0].textContent?.trim() || 
                                   timeElements[0].getAttribute('datetime') || '';
                    }
                    
                    // å°è¯•æå–ç‚¹èµæ•°
                    let likeCount = 0;
                    const likeSelectors = '[class*="like"], [class*="heart"], [class*="thumb"]';
                    const likeElements = element.querySelectorAll(likeSelectors);
                    for (const likeEl of likeElements) {
                        const likeText = likeEl.textContent?.trim() || '';
                        const match = likeText.match(/\d+/);
                        if (match) {
                            likeCount = parseInt(match[0]);
                            break;
                        }
                    }
                    
                    const comment = {
                        id: `comment_${index}_${Date.now()}`,
                        content: textContent,
                        username: username,
                        timestamp: timestamp,
                        like_count: likeCount,
                        element_class: element.className || '',
                        element_tag: element.tagName,
                        element_id: element.id || '',
                        extracted_at: new Date().toISOString()
                    };
                    
                    comments.push(comment);
                    
                } catch (error) {
                    debugInfo.errors.push(`æå–è¯„è®º ${index} æ—¶å‡ºé”™: ${error.message}`);
                }
            });
            
            // å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
            const uniqueComments = [];
            const seenContent = new Set();
            
            for (const comment of comments) {
                const contentKey = comment.content.substring(0, 30); // ä½¿ç”¨å‰30ä¸ªå­—ç¬¦ä½œä¸ºå»é‡ä¾æ®
                if (!seenContent.has(contentKey) && comment.content.length > 3) {
                    seenContent.add(contentKey);
                    uniqueComments.push(comment);
                }
            }
            
            debugInfo.finalComments = uniqueComments;
            
            return {
                comments: uniqueComments,
                debug: debugInfo
            };
        })();
        """
        
        try:
            result = await page.evaluate(js_code)
            comments = result['comments']
            debug_info = result['debug']
            
            # è¾“å‡ºè¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            self.log(f"é€‰æ‹©å™¨æœç´¢ç»“æœ:")
            for search in debug_info['searchResults']:
                self.log(f"  - {search['selector']}: æ‰¾åˆ° {search['found']} ä¸ªå…ƒç´ ")
                for sample in search['samples']:
                    self.log(f"    * {sample['tagName']}.{sample['className']}: '{sample['textPreview']}'...")
            
            if debug_info['errors']:
                self.log("æå–è¿‡ç¨‹ä¸­çš„é”™è¯¯:")
                for error in debug_info['errors']:
                    self.log(f"  - {error}", "ERROR")
            
            self.log(f"æˆåŠŸæå–åˆ° {len(comments)} æ¡è¯„è®º")
            
            # æ˜¾ç¤ºå‰å‡ æ¡è¯„è®ºçš„è¯¦ç»†ä¿¡æ¯
            if comments:
                self.log("å‰3æ¡è¯„è®ºè¯¦æƒ…:")
                for i, comment in enumerate(comments[:3], 1):
                    self.log(f"  è¯„è®º {i}:")
                    self.log(f"    ç”¨æˆ·: {comment.get('username', 'æœªçŸ¥')}")
                    self.log(f"    å†…å®¹: {comment['content'][:100]}...")
                    self.log(f"    å…ƒç´ ç±»å‹: {comment.get('element_tag', 'unknown')}")
                    self.log(f"    CSSç±»: {comment.get('element_class', 'none')}")
            
            return comments
            
        except Exception as e:
            self.log(f"æå–è¯„è®ºæ—¶å‡ºé”™: {e}", "ERROR")
            return []
    
    async def take_screenshot(self, page, filename):
        """æˆªå›¾ä¿å­˜ç”¨äºè°ƒè¯•"""
        try:
            screenshot_path = f"/Users/ankanghao/AiProjects/coze_study/xhs/{filename}"
            await page.screenshot(path=screenshot_path, full_page=True)
            self.log(f"æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
        except Exception as e:
            self.log(f"æˆªå›¾å¤±è´¥: {e}", "ERROR")
    
    async def scrape_comments(self, url: str, cookies_path: str = None, max_scrolls: int = 30):
        """çˆ¬å–æŒ‡å®šURLçš„è¯„è®º"""
        try:
            async with async_playwright() as playwright:
                self.log("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
                browser = await playwright.chromium.launch(
                    headless=self.headless,
                    args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
                )
                
                context = await browser.new_context(
                    user_agent=self.user_agent,
                    viewport={'width': 1280, 'height': 900},
                    locale='zh-CN'
                )
                
                # åŠ è½½cookies
                cookies = self.load_cookies(cookies_path)
                if cookies:
                    try:
                        await context.add_cookies(cookies)
                        self.log("âœ… å·²æˆåŠŸå¯¼å…¥cookies")
                    except Exception as e:
                        self.log(f"âŒ å¯¼å…¥cookieså¤±è´¥: {e}", "ERROR")
                        self.log("   ç»§ç»­å°è¯•æ— cookiesè®¿é—®...")
                
                page = await context.new_page()
                page.set_default_timeout(self.timeout * 1000)
                
                self.log(f"æ­£åœ¨è®¿é—®é¡µé¢: {url}")
                await page.goto(url, wait_until="domcontentloaded")
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                self.log("ç­‰å¾…é¡µé¢åŠ è½½...")
                await page.wait_for_timeout(3000)
                
                # æ£€æŸ¥ç™»å½•çŠ¶æ€
                is_logged_in = await self.check_login_status(page)
                if not is_logged_in:
                    self.log("âŒ æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼Œå¯èƒ½éœ€è¦æœ‰æ•ˆçš„cookies", "WARNING")
                
                # åˆ†æé¡µé¢ç»“æ„
                await self.analyze_page_structure(page)
                
                # æˆªå›¾ä¿å­˜å½“å‰çŠ¶æ€
                await self.take_screenshot(page, "page_initial.png")
                
                # æ»šåŠ¨åŠ è½½æ›´å¤šè¯„è®º
                await self.scroll_and_load_comments(page, max_rounds=max_scrolls)
                
                # æˆªå›¾ä¿å­˜æ»šåŠ¨åçŠ¶æ€
                await self.take_screenshot(page, "page_after_scroll.png")
                
                # æå–è¯„è®ºæ•°æ®
                comments = await self.extract_comments(page)
                
                await context.close()
                await browser.close()
                
                return comments
                
        except Exception as e:
            self.log(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}", "ERROR")
            return []
    
    def save_comments(self, comments, output_file, url):
        """ä¿å­˜è¯„è®ºæ•°æ®åˆ°æ–‡ä»¶"""
        note_id = self.extract_note_id(url)
        
        result_data = {
            "note_id": note_id,
            "url": url,
            "scraped_at": datetime.now().isoformat(),
            "comment_count": len(comments),
            "comments": comments,
            "scraper_version": "v2.1_with_cookies"
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        self.log(f"æˆåŠŸä¿å­˜ {len(comments)} æ¡è¯„è®ºåˆ°: {output_file}")
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªçº¯æ–‡æœ¬ç‰ˆæœ¬æ–¹ä¾¿æŸ¥çœ‹
        txt_file = output_file.replace('.json', '.txt')
        with open(txt_file, "w", encoding="utf-8") as f:
            f.write(f"å°çº¢ä¹¦ç¬”è®°è¯„è®º - {note_id}\n")
            f.write(f"URL: {url}\n")
            f.write(f"çˆ¬å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¯„è®ºæ€»æ•°: {len(comments)}\n")
            f.write("=" * 50 + "\n\n")
            
            for i, comment in enumerate(comments, 1):
                f.write(f"{i}. {comment.get('username', 'åŒ¿åç”¨æˆ·')}\n")
                f.write(f"   æ—¶é—´: {comment.get('timestamp', 'æœªçŸ¥')}\n")
                f.write(f"   å†…å®¹: {comment['content']}\n")
                f.write(f"   ç‚¹èµ: {comment.get('like_count', 0)}\n")
                f.write(f"   å…ƒç´ ä¿¡æ¯: {comment.get('element_tag', 'unknown')}.{comment.get('element_class', 'none')}\n")
                f.write("-" * 30 + "\n")
        
        self.log(f"åŒæ—¶ä¿å­˜æ–‡æœ¬ç‰ˆæœ¬åˆ°: {txt_file}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    config = {
        "url": "https://www.xiaohongshu.com/discovery/item/68982712000000002500d698?source=webshare&xhsshare=pc_web&xsec_token=ABQ8B9Q9MRNQCx1O5MA3-xNhA3h96U30ZFxvX5eWKkGtQ=&xsec_source=pc_share",
        "output_file": "/Users/ankanghao/AiProjects/coze_study/xhs/code/comments_data_with_cookies.json",
        "cookies_path": "/Users/ankanghao/AiProjects/coze_study/xhs/code/cookies_template.json",  # è®¾ç½®cookiesæ–‡ä»¶è·¯å¾„
        "headless": False,     # éæ— å¤´æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
        "max_scrolls": 20,     # å‡å°‘æ»šåŠ¨æ¬¡æ•°ç”¨äºè°ƒè¯•
        "debug": True          # å¯ç”¨è°ƒè¯•æ¨¡å¼
    }
    
    print("=" * 60)
    print("ğŸ”¥ å°çº¢ä¹¦è¯„è®ºçˆ¬å–å·¥å…· (Cookiesç‰ˆ)")
    print("=" * 60)
    print(f"ç›®æ ‡URL: {config['url']}")
    print(f"è¾“å‡ºæ–‡ä»¶: {config['output_file']}")
    print(f"Cookiesæ–‡ä»¶: {config['cookies_path']}")
    print(f"è°ƒè¯•æ¨¡å¼: {config['debug']}")
    print(f"æ— å¤´æ¨¡å¼: {config['headless']}")
    print("=" * 60)
    
    # æ£€æŸ¥cookiesæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config['cookies_path']):
        print(f"âŒ Cookiesæ–‡ä»¶ä¸å­˜åœ¨: {config['cookies_path']}")
        print("\nğŸ“‹ è·å–Cookiesçš„æ­¥éª¤:")
        print("1. è¿è¡Œ cookie_extractor.py è„šæœ¬")
        print("2. æˆ–è€…æ‰‹åŠ¨ä»æµè§ˆå™¨å¯¼å‡ºcookies")
        print("3. å°†cookiesä¿å­˜ä¸ºJSONæ ¼å¼")
        return
    
    # åˆ›å»ºçˆ¬å–å™¨å®ä¾‹
    scraper = XHSCommentScraper(
        headless=config["headless"], 
        debug=config["debug"]
    )
    
    try:
        # æ‰§è¡Œçˆ¬å–
        comments = asyncio.run(scraper.scrape_comments(
            url=config["url"],
            cookies_path=config["cookies_path"],
            max_scrolls=config["max_scrolls"]
        ))
        
        if comments:
            # ä¿å­˜ç»“æœ
            scraper.save_comments(comments, config["output_file"], config["url"])
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
            print(f"   æ€»è¯„è®ºæ•°: {len(comments)}")
            
            # æ˜¾ç¤ºå‰5æ¡è¯„è®ºé¢„è§ˆ
            print("\nğŸ“ è¯„è®ºé¢„è§ˆ (å‰5æ¡):")
            for i, comment in enumerate(comments[:5], 1):
                content = comment['content'][:50] + "..." if len(comment['content']) > 50 else comment['content']
                print(f"   {i}. {comment.get('username', 'åŒ¿å')}: {content}")
        else:
            print("âŒ æœªèƒ½çˆ¬å–åˆ°ä»»ä½•è¯„è®º")
            print("\nğŸ” è°ƒè¯•å»ºè®®:")
            print("1. æ£€æŸ¥cookiesæ˜¯å¦æœ‰æ•ˆå’Œå®Œæ•´")
            print("2. é‡æ–°è·å–æœ€æ–°çš„cookies")
            print("3. æ£€æŸ¥ç”Ÿæˆçš„æˆªå›¾æ–‡ä»¶äº†è§£é¡µé¢çŠ¶æ€")
            print("4. ç¡®è®¤ç¬”è®°URLæ˜¯å¦æ­£ç¡®ä¸”å¯è®¿é—®")
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 